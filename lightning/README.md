# [âš¡lightning] How to train or inference in CLI? [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1CzPRYS35xKfNrxidSVhDiMMQulqud9yu?usp=share_link) [![wandb](https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-gradient.svg)](https://wandb.ai/wako/Korean_Summarization?workspace=user-wako)
 

## Check Jupyter Notebook Version (Just âš¡lightning_style Baseline) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Uq1sgNtez99AmIXccOMHhpfVCNmoDLeQ?usp=share_link) 
 
here gonna be img soon

## Notice
 - Unfortunately, Data is not Provided.
 - Tried âš¡lightning 2.0 for the following reasons:
   - Compare with `ğŸ¤—huggingface` or `ğŸ”¥torch`.
   - compatible for `torch.compile()`: however, got error whenever I tried. 
     - I guess that`peft` might be not compatible with `ğŸ”¥Pytorch 2.0`.
   - Just for studying `âš¡lightning 2.0` and `Fabric`. 
- This code costs a lot of time for training.
- In this code, you can check ROUGE scores after each step during `validation`.
    - I just wanna visualize the `ROUGE` scores per epoch.
    - You can set the number of sentences for `ROUGE` scores after each step. (`num_sentences`)
    
   
## [wandb login in CLI interface](https://docs.wandb.ai/ref/cli/wandb-login)
```python
$ wandb login --relogin '######### your API token ###########'                  
``` 


## Train 
```bash
$ python train.py --data_path '/content/drive/MyDrive/ ... /data/' \
                  --model_save '/content/drive/MyDrive/ ... /test/model/l_bart_lora/' \
                  --is_sample True \
                  --sample 1000 \
                  --model 'gogamza/kobart-base-v2' \
                  --is_lora True \
                  --is_compiled False \
                  --lora_r 4 \
                  --lora_alpha 32 \
                  --lora_target_modules "['q_proj', 'v_proj']" \
                  --lora_dropout_p 0.05 \
                  --try_title "_bart_lora" \
                  --n_epochs 2 \
                  --num_sentences 2 \
                  --max_length 512 \
                  --target_max_length 55 \
                  --train_batch_size 8 \
                  --valid_batch_size 8
```
- `data_path` : Dataê°€ ì €ì¥ëœ ê²½ë¡œ (Default: `./data/`)
- `model_save`: í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ë˜ëŠ” ê²½ë¡œ
- `model`: Huggingface Pratrained Model (Default: `"eenzeenee/t5-small-korean-summarization"`)
- `is_lora` : LoRA ì ìš© ì—¬ë¶€ `True` or `False`
- `is_compiled` : `torch.compile()` ì ìš© ì—¬ë¶€ `True` or `False` (ì´ë²ˆ íƒœìŠ¤íŠ¸ì—ì„œëŠ” `torch.compile()`ì„ ì§„í–‰í•˜ë©´ ì—ëŸ¬ê°€ ë‚˜ì„œ Defaultë¡œ`False`ë¡œ í•´ë‘ì—ˆë‹¤.)
- `lora_r`, `lora_alpha`, `lora_target_modules`, `lora_dropout` : LoRA CONFIG
- `n_epochs` : Epoch
- `num_sentences` : `train` í˜¹ì€ `validation` ë™ì•ˆ `Rouge` ìŠ¤ì½”ì–´ë¥¼ ë‚¼ ë¬¸ì¥ì˜ ê°œìˆ˜ (Batch Sizeë³´ë‹¤ëŠ” ì ì€ ë¬¸ì¥ ìˆ˜)
- [`train.py`](https://github.com/renslightsaber/korean_summarization_competition/blob/main/lightning/train.py) ì°¸ê³ !   


## Inference 
```bash
$ python inference.py --data_path '/content/drive/MyDrive/ ... /data/' \
                      --model_save '/content/drive/MyDrive/ ... /test/model/l_bart_lora/' \
                      --sub_path '/content/drive/MyDrive/ ... /test/sub/' \
                      --model 'gogamza/kobart-base-v2' \
                      --is_lora True \
                      --lora_r 4 \
                      --lora_alpha 32 \
                      --lora_target_modules "['q_proj', 'v_proj']" \
                      --lora_dropout_p 0.05 \
                      --try_title "test_bart_lora" \
                      --num_beams 3 \
                      --max_length 512 \
                      --target_max_length 55 \
                      --valid_batch_size 8
                      
```
- `base_path` : Dataê°€ ì €ì¥ëœ ê²½ë¡œ (Default: `./data/`)
- `sub_path`  : `submission.csv` ì œì¶œí•˜ëŠ” ê²½ë¡œ
- [`inference.py`](https://github.com/renslightsaber/korean_summarization_competition/blob/main/lightning/inference.py) ì°¸ê³ !   

